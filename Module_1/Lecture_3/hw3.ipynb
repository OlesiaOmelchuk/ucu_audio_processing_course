{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import re\n",
    "import scipy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import nnAudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from nnAudio.features.mel import MelSpectrogram\n",
    "from torchaudio.transforms import AmplitudeToDB\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, \n",
    "    AutoProcessor, Wav2Vec2Model, HubertForCTC, \n",
    "    HubertModel, Data2VecAudioForCTC, Data2VecAudioModel,\n",
    "    AutoModelForAudioXVector, Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\famil\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for timit_asr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/timit_asr\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('timit_asr', data_dir=\"../../data/TIMIT\")\n",
    "ds_train, ds_test = ds['train'], ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'c:\\\\DATA\\\\UCU\\\\YEAR_3\\\\AUDIO\\\\ucu_audio_processing_course\\\\data\\\\TIMIT\\\\data\\\\lisa\\\\data\\\\timit\\\\raw\\\\TIMIT\\\\train\\\\DR1\\\\FCJF0\\\\SA1.WAV',\n",
       " 'audio': {'path': 'c:\\\\DATA\\\\UCU\\\\YEAR_3\\\\AUDIO\\\\ucu_audio_processing_course\\\\data\\\\TIMIT\\\\data\\\\lisa\\\\data\\\\timit\\\\raw\\\\TIMIT\\\\train\\\\DR1\\\\FCJF0\\\\SA1.WAV',\n",
       "  'array': array([ 3.05175781e-05, -3.05175781e-05,  6.10351562e-05, ...,\n",
       "         -3.05175781e-05, -1.52587891e-04, -2.44140625e-04]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': 'She had your dark suit in greasy wash water all year.',\n",
       " 'phonetic_detail': {'start': [0,\n",
       "   3050,\n",
       "   4559,\n",
       "   5723,\n",
       "   6642,\n",
       "   8772,\n",
       "   9190,\n",
       "   10337,\n",
       "   11517,\n",
       "   12500,\n",
       "   12640,\n",
       "   14714,\n",
       "   15870,\n",
       "   16334,\n",
       "   18088,\n",
       "   20417,\n",
       "   21199,\n",
       "   22560,\n",
       "   22920,\n",
       "   23271,\n",
       "   24229,\n",
       "   25566,\n",
       "   27156,\n",
       "   28064,\n",
       "   29660,\n",
       "   31719,\n",
       "   33360,\n",
       "   33754,\n",
       "   34715,\n",
       "   36080,\n",
       "   36326,\n",
       "   37556,\n",
       "   39561,\n",
       "   40313,\n",
       "   42059,\n",
       "   43479,\n",
       "   44586],\n",
       "  'stop': [3050,\n",
       "   4559,\n",
       "   5723,\n",
       "   6642,\n",
       "   8772,\n",
       "   9190,\n",
       "   10337,\n",
       "   11517,\n",
       "   12500,\n",
       "   12640,\n",
       "   14714,\n",
       "   15870,\n",
       "   16334,\n",
       "   18088,\n",
       "   20417,\n",
       "   21199,\n",
       "   22560,\n",
       "   22920,\n",
       "   23271,\n",
       "   24229,\n",
       "   25566,\n",
       "   27156,\n",
       "   28064,\n",
       "   29660,\n",
       "   31719,\n",
       "   33360,\n",
       "   33754,\n",
       "   34715,\n",
       "   36080,\n",
       "   36326,\n",
       "   37556,\n",
       "   39561,\n",
       "   40313,\n",
       "   42059,\n",
       "   43479,\n",
       "   44586,\n",
       "   46720],\n",
       "  'utterance': ['h#',\n",
       "   'sh',\n",
       "   'ix',\n",
       "   'hv',\n",
       "   'eh',\n",
       "   'dcl',\n",
       "   'jh',\n",
       "   'ih',\n",
       "   'dcl',\n",
       "   'd',\n",
       "   'ah',\n",
       "   'kcl',\n",
       "   'k',\n",
       "   's',\n",
       "   'ux',\n",
       "   'q',\n",
       "   'en',\n",
       "   'gcl',\n",
       "   'g',\n",
       "   'r',\n",
       "   'ix',\n",
       "   's',\n",
       "   'ix',\n",
       "   'w',\n",
       "   'ao',\n",
       "   'sh',\n",
       "   'epi',\n",
       "   'w',\n",
       "   'ao',\n",
       "   'dx',\n",
       "   'axr',\n",
       "   'ao',\n",
       "   'l',\n",
       "   'y',\n",
       "   'ih',\n",
       "   'axr',\n",
       "   'h#']},\n",
       " 'word_detail': {'start': [3050,\n",
       "   5723,\n",
       "   9190,\n",
       "   11517,\n",
       "   16334,\n",
       "   21199,\n",
       "   22560,\n",
       "   28064,\n",
       "   33754,\n",
       "   37556,\n",
       "   40313],\n",
       "  'stop': [5723,\n",
       "   10337,\n",
       "   11517,\n",
       "   16334,\n",
       "   21199,\n",
       "   22560,\n",
       "   28064,\n",
       "   33360,\n",
       "   37556,\n",
       "   40313,\n",
       "   44586],\n",
       "  'utterance': ['she',\n",
       "   'had',\n",
       "   'your',\n",
       "   'dark',\n",
       "   'suit',\n",
       "   'in',\n",
       "   'greasy',\n",
       "   'wash',\n",
       "   'water',\n",
       "   'all',\n",
       "   'year']},\n",
       " 'dialect_region': 'DR1',\n",
       " 'sentence_type': 'SA',\n",
       " 'speaker_id': 'CJF0',\n",
       " 'id': 'SA1'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train phones:\t 61\n",
      "num of train phones:\t 61\n"
     ]
    }
   ],
   "source": [
    "train_phonetics = [phone for x in ds_train for phone in x['phonetic_detail']['utterance']]\n",
    "print(\"num of train phones:\\t\", len(set(train_phonetics)))\n",
    "\n",
    "test_phonetics = [phone for x in ds_test for phone in x['phonetic_detail']['utterance']]\n",
    "print(\"num of train phones:\\t\", len(set(test_phonetics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://www.kaggle.com/code/vitouphy/phoneme-recognition-with-wav2vec2\n",
    "phon61_map = {\n",
    "    'iy':'iy',  'ih':'ih',   'eh':'eh',  'ae':'ae',    'ix':'ih',  'ax':'ah',   'ah':'ah',  'uw':'uw',\n",
    "    'ux':'uw',  'uh':'uh',   'ao':'aa',  'aa':'aa',    'ey':'ey',  'ay':'ay',   'oy':'oy',  'aw':'aw',\n",
    "    'ow':'ow',  'l':'l',     'el':'l',  'r':'r',      'y':'y',    'w':'w',     'er':'er',  'axr':'er',\n",
    "    'm':'m',    'em':'m',     'n':'n',    'nx':'n',     'en':'n',  'ng':'ng',   'eng':'ng', 'ch':'ch',\n",
    "    'jh':'jh',  'dh':'dh',   'b':'b',    'd':'d',      'dx':'dx',  'g':'g',     'p':'p',    't':'t',\n",
    "    'k':'k',    'z':'z',     'zh':'sh',  'v':'v',      'f':'f',    'th':'th',   's':'s',    'sh':'sh',\n",
    "    'hh':'hh',  'hv':'hh',   'pcl':'h#', 'tcl':'h#', 'kcl':'h#', 'qcl':'h#','bcl':'h#','dcl':'h#',\n",
    "    'gcl':'h#','h#':'h#',  '#h':'h#',  'pau':'h#', 'epi': 'h#','nx':'n',   'ax-h':'ah','q':'h#',\n",
    "        \"a\": \"ə\", \"ey\": \"eɪ\", \"aa\": \"ɑ\", \"ae\": \"æ\", \"ah\": \"ə\", \"ao\": \"ɔ\",\n",
    "        \"aw\": \"aʊ\", \"ay\": \"aɪ\", \"ch\": \"ʧ\", \"dh\": \"ð\", \"eh\": \"ɛ\", \"er\": \"ər\",\n",
    "        \"hh\": \"h\", \"ih\": \"ɪ\", \"jh\": \"ʤ\", \"ng\": \"ŋ\",  \"ow\": \"oʊ\", \"oy\": \"ɔɪ\",\n",
    "        \"sh\": \"ʃ\", \"th\": \"θ\", \"uh\": \"ʊ\", \"uw\": \"u\", \"zh\": \"ʒ\", \"iy\": \"i\", \"y\": \"j\"\n",
    "}\n",
    "\n",
    "def convert_phon61_to_phon39(sentence):\n",
    "    # tokens = [phon61_map[x] for x in sentence]\n",
    "    # return \" \".join(tokens)\n",
    "    return [phon61_map[x] for x in sentence]\n",
    "\n",
    "def normalize_phones(item):\n",
    "    item['phonetic_detail']['utterance'] = convert_phon61_to_phon39(item['phonetic_detail']['utterance'])\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_norm = ds_train.map(normalize_phones)\n",
    "ds_test_norm = ds_test.map(normalize_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train phones:\t 47\n",
      "num of test phones:\t 47\n"
     ]
    }
   ],
   "source": [
    "train_phonetics_norm = [phone for x in ds_train_norm for phone in x['phonetic_detail']['utterance']]\n",
    "print(\"num of train phones:\\t\", len(set(train_phonetics_norm)))\n",
    "\n",
    "test_phonetics_norm = [phone for x in ds_test_norm for phone in x['phonetic_detail']['utterance']]\n",
    "print(\"num of test phones:\\t\", len(set(test_phonetics_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNK', 'ah', 'aɪ', 'aʊ', 'b', 'd', 'dx', 'er', 'eɪ', 'f', 'g', 'h', 'h#', 'hh', 'i', 'ih', 'j', 'k', 'l', 'm', 'n', 'ng', 'oʊ', 'p', 'r', 's', 't', 'u', 'uw', 'v', 'w', 'z', 'æ', 'ð', 'ŋ', 'ɑ', 'ɔ', 'ɔɪ', 'ə', 'ər', 'ɛ', 'ɪ', 'ʃ', 'ʊ', 'ʒ', 'ʤ', 'ʧ', 'θ']\n"
     ]
    }
   ],
   "source": [
    "vocab_list = sorted(list(set(train_phonetics_norm + test_phonetics_norm + [\"UNK\"])))\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_encoder = LabelEncoder().fit(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 42, 15, 13, 40, 12, 45, 41, 12,  5, 38, 12, 17, 25, 28, 12, 20,\n",
       "       12, 10, 24, 15, 25, 15, 30, 36, 42, 12, 30, 36,  6,  7, 36, 18, 16,\n",
       "       41,  7, 12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_label_encoder.transform(ds_train_norm[0][\"phonetic_detail\"][\"utterance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [target_label_encoder.transform(ds_train_norm[i][\"phonetic_detail\"][\"utterance\"]) for i in range(len(ds_train_norm))]\n",
    "test_y = [target_label_encoder.transform(ds_test_norm[i][\"phonetic_detail\"][\"utterance\"]) for i in range(len(ds_test_norm))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(entry):\n",
    "    entry[\"labels\"] = target_label_encoder.transform(entry[\"phonetic_detail\"][\"utterance\"])\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_norm = ds_train_norm.map(add_labels)\n",
    "ds_test_norm = ds_test_norm.map(add_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12,\n",
       " 42,\n",
       " 15,\n",
       " 13,\n",
       " 40,\n",
       " 12,\n",
       " 45,\n",
       " 41,\n",
       " 12,\n",
       " 5,\n",
       " 38,\n",
       " 12,\n",
       " 17,\n",
       " 25,\n",
       " 28,\n",
       " 12,\n",
       " 20,\n",
       " 12,\n",
       " 10,\n",
       " 24,\n",
       " 15,\n",
       " 25,\n",
       " 15,\n",
       " 30,\n",
       " 36,\n",
       " 42,\n",
       " 12,\n",
       " 30,\n",
       " 36,\n",
       " 6,\n",
       " 7,\n",
       " 36,\n",
       " 18,\n",
       " 16,\n",
       " 41,\n",
       " 7,\n",
       " 12]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_norm[0][\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metric (PER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_PER(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate Phoneme Error Rate (PER) between predicted and true phoneme sequences.    \n",
    "    \"\"\"\n",
    "    n = len(reference)\n",
    "    m = len(hypothesis)\n",
    "\n",
    "    D = [[0] * (m + 1) for _ in range(n + 1)]\n",
    "    \n",
    "    for i in range(n + 1):\n",
    "        D[i][0] = i\n",
    "    for j in range(m + 1):\n",
    "        D[0][j] = j\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            if reference[i - 1] == hypothesis[j - 1]:\n",
    "                D[i][j] = D[i - 1][j - 1]\n",
    "            else:\n",
    "                D[i][j] = min(D[i - 1][j] + 1,  # deletion\n",
    "                              D[i][j - 1] + 1,  # insertion\n",
    "                              D[i - 1][j - 1] + 1)  # substitution\n",
    "    \n",
    "    return D[n][m] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_PER(predict_list, true_list):\n",
    "    return np.mean([calculate_PER(predict_list[i], true_list[i]) for i in range(len(predict_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme Error Rate: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "reference_phonemes = [1, 2, 1]\n",
    "hypothesis_phonemes = [1, 2, 3, 4]\n",
    "PER = calculate_PER(reference_phonemes, hypothesis_phonemes)\n",
    "print(\"Phoneme Error Rate:\", PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thanks god it finally works\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "_ESPEAK_LIBRARY = 'C:\\Program Files\\eSpeak NG\\libespeak-ng.dll'\n",
    "EspeakWrapper.set_library(_ESPEAK_LIBRARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "input_values = processor(ds_train[0][\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(input_values, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0347, -0.0542,  0.0501,  ...,  0.0710, -0.0322,  0.0433],\n",
       "        [-0.0329, -0.0493,  0.0476,  ...,  0.0611, -0.0296,  0.0419],\n",
       "        [-0.0378, -0.0434,  0.0737,  ...,  0.0549, -0.0293,  0.0383],\n",
       "        ...,\n",
       "        [-0.0236, -0.0921, -0.0580,  ...,  0.0870, -0.0522,  0.0427],\n",
       "        [-0.0248, -0.0752, -0.1010,  ...,  0.1145, -0.0296,  0.0333],\n",
       "        [-0.0458, -0.0513,  0.0274,  ...,  0.0900, -0.0200,  0.0343]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She had your dark suit in greasy wash water all year.\n",
      "h# ʃ ih hh ɛ h# ʤ ɪ h# d ə h# k s uw h# n h# g r ih s ih w ɔ ʃ h# w ɔ dx er ɔ l j ɪ er h#\n",
      "['ʃ iː h æ dʒ ɚ d ʌ k s uː t æ n ɡ ɹ iː s i w ɑː ʃ w ɔː ɾ ɚ ɹ ɔː l j ɪɹ']\n"
     ]
    }
   ],
   "source": [
    "print(ds_train['text'][0])\n",
    "print(ds_train_norm['phonetic_detail'][0]['utterance'])\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_unk = [\"UNK\" if x not in vocab_list else x for x in transcription[0].split()]\n",
    "pred_y = target_label_encoder.transform(transcription_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7567567567567568"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = calculate_PER(train_y[0], pred_y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hidden_state(entry):\n",
    "    input_values = processor(entry[\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values, output_hidden_states=True)\n",
    "    entry[\"embs\"] = outputs.hidden_states[-1][0]\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_small = ds_train_norm.select(range(200))\n",
    "ds_test_small = ds_test_norm.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function add_hidden_state at 0x000002C05E70D3A0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45e906c57c3438dbf75740e4516c504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f21629ae8f04dbe8fd6396321f7d2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_train_small = ds_train_small.map(add_hidden_state)\n",
    "ds_test_small = ds_test_small.map(add_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embs = ds_train_small[\"embs\"]\n",
    "train_y_small = ds_train_small[\"labels\"]\n",
    "\n",
    "test_embs = ds_test_small[\"embs\"]\n",
    "test_y_small = ds_test_small[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_phonems(ds, sr=16000):\n",
    "    phonemes = []\n",
    "    for i in range(len(ds)):\n",
    "        input_values = processor(ds[i][\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=sr).input_values\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)\n",
    "        transcription_unk = [\"UNK\" if x not in vocab_list else x for x in transcription[0].split()]\n",
    "        pred_y = target_label_encoder.transform(transcription_unk)\n",
    "        phonemes.append(pred_y)\n",
    "    return phonemes\n",
    "\n",
    "def pad_phonems(phonems):\n",
    "    max_length = max(len(sublist) for sublist in phonems)\n",
    "    X_padded = [list(sublist) + [0] * (max_length - len(sublist)) for sublist in phonems]\n",
    "    return np.array(X_padded)\n",
    "    # return torch.cat(X_padded)\n",
    "\n",
    "def pad_embs(phonems):\n",
    "    max_length = max(len(sublist) for sublist in phonems)\n",
    "    X_padded = [list(sublist) + [[0] * 1024] * (max_length - len(sublist)) for sublist in phonems]\n",
    "    return np.array(X_padded)\n",
    "    # return torch.cat(X_padded, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = pad_embs(train_embs)\n",
    "train_y_small_pad = pad_phonems(train_y_small)\n",
    "\n",
    "test_w2v = pad_embs(test_embs)\n",
    "test_y_small_pad = pad_phonems(test_y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 318, 1024)\n",
      "(200, 68)\n",
      "(100, 371, 1024)\n",
      "(100, 71)\n"
     ]
    }
   ],
   "source": [
    "print(train_w2v.shape)\n",
    "print(train_y_small_pad.shape)\n",
    "\n",
    "print(test_w2v.shape)\n",
    "print(test_y_small_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dublicate_y(arr_y, arr_embs):\n",
    "    _, b = arr_y.shape\n",
    "    _, c, _ = arr_embs.shape\n",
    "    rows_to_repeat = c // b \n",
    "    arr_duplicated = np.repeat(arr_y, rows_to_repeat, axis=1)\n",
    "    extra_zeros = c - arr_duplicated.shape[1]\n",
    "    arr2_duplicated_pad = np.pad(arr_duplicated, ((0, 0), (0, extra_zeros)), mode='constant', constant_values=0)\n",
    "    return arr2_duplicated_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 12, 12, 12, 42, 42, 42, 42, 15, 15, 15, 15, 13, 13, 13, 13, 40,\n",
       "       40, 40, 40, 12, 12, 12, 12, 45, 45, 45, 45, 41, 41, 41, 41, 12, 12,\n",
       "       12, 12,  5,  5,  5,  5, 38, 38, 38, 38, 12, 12, 12, 12, 17, 17, 17,\n",
       "       17, 25, 25, 25, 25, 28, 28, 28, 28, 12, 12, 12, 12, 20, 20, 20, 20,\n",
       "       12, 12, 12, 12, 10, 10, 10, 10, 24, 24, 24, 24, 15, 15, 15, 15, 25,\n",
       "       25, 25, 25, 15, 15, 15, 15, 30, 30, 30, 30, 36, 36, 36, 36, 42, 42,\n",
       "       42, 42, 12, 12, 12, 12, 30, 30, 30, 30, 36, 36, 36, 36,  6,  6,  6,\n",
       "        6,  7,  7,  7,  7, 36, 36, 36, 36, 18, 18, 18, 18, 16, 16, 16, 16,\n",
       "       41, 41, 41, 41,  7,  7,  7,  7, 12, 12, 12, 12,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = dublicate_y(train_y_small_pad, train_w2v)\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_embs_and_y(arr_y, arr_embs):\n",
    "    \"Reshape 3d array of last hiddent states and 2d array of phones correspondingly\"\n",
    "    arr_y_dub = dublicate_y(arr_y, arr_embs)\n",
    "    return arr_embs.reshape(-1, arr_embs.shape[-1]), arr_y_dub.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v_ready, train_y_ready = transform_embs_and_y(train_y_small_pad, train_w2v)\n",
    "test_w2v_ready, test_y_ready = transform_embs_and_y(test_y_small_pad, test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\famil\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;logreg&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;logreg&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('logreg', LogisticRegression())])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_pipeline.fit(train_w2v_ready, train_y_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train PER score:  0.0863235294117647\n"
     ]
    }
   ],
   "source": [
    "train_w2v2_predict = classification_pipeline.predict(train_w2v_ready)\n",
    "print(\"Train PER score: \", calculate_PER(train_w2v2_predict.tolist(), train_y_ready.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w2v2_predict = classification_pipeline.predict(test_w2v_ready)\n",
    "print(\"Test PER score: \", calculate_PER(test_w2v2_predict.tolist(), test_y_ready.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_CLASSES = len(vocab_list)\n",
    "\n",
    "class PhonemeClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of the neural network, loss, optimizer\n",
    "input_size = 512\n",
    "hidden_size = 1024\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = PhonemeClassifier(input_size, hidden_size, NUM_CLASSES).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Training flow\n",
    "def train(train_loader, test_loader, model, loss_fn, optimizer):\n",
    "\n",
    "    for epoch in range(10):  # train for 10 epochs\n",
    "        train_loss = 0\n",
    "        print(\"Epoch no:\" , epoch)\n",
    "        for batch_idx, (cur_data, target) in enumerate(tqdm(train_loader)):\n",
    "            cur_data, target = cur_data.to(device), target.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            y_hat = model(cur_data)\n",
    "            loss = loss_fn(y_hat, target)\n",
    "\n",
    "            # Apply backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        print('Epoch: {}, Training Loss: {:.3f}'.format(epoch, train_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\famil\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(ds_train_norm, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(ds_test_norm, batch_size=128,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader, test_loader,  model, loss_fn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
