{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import re\n",
    "import scipy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import nnAudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from nnAudio.features.mel import MelSpectrogram\n",
    "from torchaudio.transforms import AmplitudeToDB\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, \n",
    "    AutoProcessor, Wav2Vec2Model, HubertForCTC, \n",
    "    HubertModel, Data2VecAudioForCTC, Data2VecAudioModel,\n",
    "    AutoModelForAudioXVector, Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\famil\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for timit_asr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/timit_asr\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('timit_asr', data_dir=\"../../data/TIMIT\")\n",
    "ds_train, ds_test = ds['train'], ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file': 'c:\\\\DATA\\\\UCU\\\\YEAR_3\\\\AUDIO\\\\ucu_audio_processing_course\\\\data\\\\TIMIT\\\\data\\\\lisa\\\\data\\\\timit\\\\raw\\\\TIMIT\\\\train\\\\DR1\\\\FCJF0\\\\SA1.WAV', 'audio': {'path': 'c:\\\\DATA\\\\UCU\\\\YEAR_3\\\\AUDIO\\\\ucu_audio_processing_course\\\\data\\\\TIMIT\\\\data\\\\lisa\\\\data\\\\timit\\\\raw\\\\TIMIT\\\\train\\\\DR1\\\\FCJF0\\\\SA1.WAV', 'array': array([ 3.05175781e-05, -3.05175781e-05,  6.10351562e-05, ...,\n",
      "       -3.05175781e-05, -1.52587891e-04, -2.44140625e-04]), 'sampling_rate': 16000}, 'text': 'She had your dark suit in greasy wash water all year.', 'phonetic_detail': {'start': [0, 3050, 4559, 5723, 6642, 8772, 9190, 10337, 11517, 12500, 12640, 14714, 15870, 16334, 18088, 20417, 21199, 22560, 22920, 23271, 24229, 25566, 27156, 28064, 29660, 31719, 33360, 33754, 34715, 36080, 36326, 37556, 39561, 40313, 42059, 43479, 44586], 'stop': [3050, 4559, 5723, 6642, 8772, 9190, 10337, 11517, 12500, 12640, 14714, 15870, 16334, 18088, 20417, 21199, 22560, 22920, 23271, 24229, 25566, 27156, 28064, 29660, 31719, 33360, 33754, 34715, 36080, 36326, 37556, 39561, 40313, 42059, 43479, 44586, 46720], 'utterance': ['h#', 'sh', 'ix', 'hv', 'eh', 'dcl', 'jh', 'ih', 'dcl', 'd', 'ah', 'kcl', 'k', 's', 'ux', 'q', 'en', 'gcl', 'g', 'r', 'ix', 's', 'ix', 'w', 'ao', 'sh', 'epi', 'w', 'ao', 'dx', 'axr', 'ao', 'l', 'y', 'ih', 'axr', 'h#']}, 'word_detail': {'start': [3050, 5723, 9190, 11517, 16334, 21199, 22560, 28064, 33754, 37556, 40313], 'stop': [5723, 10337, 11517, 16334, 21199, 22560, 28064, 33360, 37556, 40313, 44586], 'utterance': ['she', 'had', 'your', 'dark', 'suit', 'in', 'greasy', 'wash', 'water', 'all', 'year']}, 'dialect_region': 'DR1', 'sentence_type': 'SA', 'speaker_id': 'CJF0', 'id': 'SA1'}\n"
     ]
    }
   ],
   "source": [
    "print(ds_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train phones:\t 61\n"
     ]
    }
   ],
   "source": [
    "train_phonetics = [phone for x in ds_train for phone in x['phonetic_detail']['utterance']]\n",
    "print(\"num of train phones:\\t\", len(set(train_phonetics)))\n",
    "\n",
    "test_phonetics = [phone for x in ds_test for phone in x['phonetic_detail']['utterance']]\n",
    "print(\"num of train phones:\\t\", len(set(test_phonetics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://www.kaggle.com/code/vitouphy/phoneme-recognition-with-wav2vec2\n",
    "phon61_map = {\n",
    "    'iy':'iy',  'ih':'ih',   'eh':'eh',  'ae':'ae',    'ix':'ih',  'ax':'ah',   'ah':'ah',  'uw':'uw',\n",
    "    'ux':'uw',  'uh':'uh',   'ao':'aa',  'aa':'aa',    'ey':'ey',  'ay':'ay',   'oy':'oy',  'aw':'aw',\n",
    "    'ow':'ow',  'l':'l',     'el':'l',  'r':'r',      'y':'y',    'w':'w',     'er':'er',  'axr':'er',\n",
    "    'm':'m',    'em':'m',     'n':'n',    'nx':'n',     'en':'n',  'ng':'ng',   'eng':'ng', 'ch':'ch',\n",
    "    'jh':'jh',  'dh':'dh',   'b':'b',    'd':'d',      'dx':'dx',  'g':'g',     'p':'p',    't':'t',\n",
    "    'k':'k',    'z':'z',     'zh':'sh',  'v':'v',      'f':'f',    'th':'th',   's':'s',    'sh':'sh',\n",
    "    'hh':'hh',  'hv':'hh',   'pcl':'h#', 'tcl':'h#', 'kcl':'h#', 'qcl':'h#','bcl':'h#','dcl':'h#',\n",
    "    'gcl':'h#','h#':'h#',  '#h':'h#',  'pau':'h#', 'epi': 'h#','nx':'n',   'ax-h':'ah','q':'h#',\n",
    "        \"a\": \"ə\", \"ey\": \"eɪ\", \"aa\": \"ɑ\", \"ae\": \"æ\", \"ah\": \"ə\", \"ao\": \"ɔ\",\n",
    "        \"aw\": \"aʊ\", \"ay\": \"aɪ\", \"ch\": \"ʧ\", \"dh\": \"ð\", \"eh\": \"ɛ\", \"er\": \"ər\",\n",
    "        \"hh\": \"h\", \"ih\": \"ɪ\", \"jh\": \"ʤ\", \"ng\": \"ŋ\",  \"ow\": \"oʊ\", \"oy\": \"ɔɪ\",\n",
    "        \"sh\": \"ʃ\", \"th\": \"θ\", \"uh\": \"ʊ\", \"uw\": \"u\", \"zh\": \"ʒ\", \"iy\": \"i\", \"y\": \"j\"\n",
    "}\n",
    "\n",
    "def convert_phon61_to_phon39(sentence):\n",
    "    tokens = [phon61_map[x] for x in sentence]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def normalize_phones(item):\n",
    "    item['phonetic_detail']['utterance'] = convert_phon61_to_phon39(item['phonetic_detail']['utterance'])\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_norm = ds_train.map(normalize_phones)\n",
    "ds_test_norm = ds_test.map(normalize_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train phones:\t 47\n",
      "num of test phones:\t 47\n"
     ]
    }
   ],
   "source": [
    "train_phonetics_norm = [phone for x in ds_train_norm for phone in x['phonetic_detail']['utterance'].split()]\n",
    "print(\"num of train phones:\\t\", len(set(train_phonetics_norm)))\n",
    "\n",
    "test_phonetics_norm = [phone for x in ds_test_norm for phone in x['phonetic_detail']['utterance'].split()]\n",
    "print(\"num of test phones:\\t\", len(set(test_phonetics_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNK', 'ah', 'aɪ', 'aʊ', 'b', 'd', 'dx', 'er', 'eɪ', 'f', 'g', 'h', 'h#', 'hh', 'i', 'ih', 'j', 'k', 'l', 'm', 'n', 'ng', 'oʊ', 'p', 'r', 's', 't', 'u', 'uw', 'v', 'w', 'z', 'æ', 'ð', 'ŋ', 'ɑ', 'ɔ', 'ɔɪ', 'ə', 'ər', 'ɛ', 'ɪ', 'ʃ', 'ʊ', 'ʒ', 'ʤ', 'ʧ', 'θ']\n"
     ]
    }
   ],
   "source": [
    "vocab_list = sorted(list(set(train_phonetics_norm + test_phonetics_norm + [\"UNK\"])))\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_encoder = LabelEncoder().fit(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 42, 15, 13, 40, 12, 45, 41, 12,  5, 38, 12, 17, 25, 28, 12, 20,\n",
       "       12, 10, 24, 15, 25, 15, 30, 36, 42, 12, 30, 36,  6,  7, 36, 18, 16,\n",
       "       41,  7, 12])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_label_encoder.transform(ds_train_norm[0][\"phonetic_detail\"][\"utterance\"].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [target_label_encoder.transform(ds_train_norm[i][\"phonetic_detail\"][\"utterance\"].split()) for i in range(len(ds_train_norm))]\n",
    "test_y = [target_label_encoder.transform(ds_test_norm[i][\"phonetic_detail\"][\"utterance\"].split()) for i in range(len(ds_test_norm))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metric (PER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_PER(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate Phoneme Error Rate (PER) between predicted and true phoneme sequences.    \n",
    "    \"\"\"\n",
    "    n = len(reference)\n",
    "    m = len(hypothesis)\n",
    "\n",
    "    D = [[0] * (m + 1) for _ in range(n + 1)]\n",
    "    \n",
    "    for i in range(n + 1):\n",
    "        D[i][0] = i\n",
    "    for j in range(m + 1):\n",
    "        D[0][j] = j\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            if reference[i - 1] == hypothesis[j - 1]:\n",
    "                D[i][j] = D[i - 1][j - 1]\n",
    "            else:\n",
    "                D[i][j] = min(D[i - 1][j] + 1,  # deletion\n",
    "                              D[i][j - 1] + 1,  # insertion\n",
    "                              D[i - 1][j - 1] + 1)  # substitution\n",
    "    \n",
    "    return D[n][m] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_PER(predict_list, true_list):\n",
    "    return np.mean([calculate_PER(predict_list[i], true_list[i]) for i in range(len(predict_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme Error Rate: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "reference_phonemes = [1, 2, 1]\n",
    "hypothesis_phonemes = [1, 2, 3, 4]\n",
    "PER = calculate_PER(reference_phonemes, hypothesis_phonemes)\n",
    "print(\"Phoneme Error Rate:\", PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thanks god it finally works\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "_ESPEAK_LIBRARY = 'C:\\Program Files\\eSpeak NG\\libespeak-ng.dll'\n",
    "EspeakWrapper.set_library(_ESPEAK_LIBRARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "input_values = processor(ds_train[0][\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "with torch.no_grad():\n",
    "  logits = model(input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She had your dark suit in greasy wash water all year.\n",
      "h# ʃ ih hh ɛ h# ʤ ɪ h# d ə h# k s uw h# n h# g r ih s ih w ɔ ʃ h# w ɔ dx er ɔ l j ɪ er h#\n",
      "['ʃ iː h æ dʒ ɚ d ʌ k s uː t æ n ɡ ɹ iː s i w ɑː ʃ w ɔː ɾ ɚ ɹ ɔː l j ɪɹ']\n"
     ]
    }
   ],
   "source": [
    "print(ds_train['text'][0])\n",
    "print(ds_train_norm['phonetic_detail'][0]['utterance'])\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_unk = [\"UNK\" if x not in vocab_list else x for x in transcription[0].split()]\n",
    "pred_y = target_label_encoder.transform(transcription_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7567567567567568"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = calculate_PER(train_y[0], pred_y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_phonems(ds, sr=16000):\n",
    "    phonemes = []\n",
    "    for i in range(len(ds)):\n",
    "        input_values = processor(ds[i][\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=sr).input_values\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)\n",
    "        transcription_unk = [\"UNK\" if x not in vocab_list else x for x in transcription[0].split()]\n",
    "        pred_y = target_label_encoder.transform(transcription_unk)\n",
    "        phonemes.append(pred_y)\n",
    "    return phonemes\n",
    "\n",
    "def pad_phonems(phonems):\n",
    "    max_length = max(len(sublist) for sublist in phonems)\n",
    "    X_padded = [torch.FloatTensor(list(sublist) + [0] * (max_length - len(sublist))) for sublist in phonems]\n",
    "    return torch.cat(X_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = predict_phonems(ds_train_norm)\n",
    "test_w2v = predict_phonems(ds_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "classification_pipeline.fit(train_w2v, train_y)\n",
    "\n",
    "train_w2v2_predict = classification_pipeline.predict(train_w2v)\n",
    "test_w2v2_predict = classification_pipeline.predict(test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8023578413995871"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train PER score: \", compute_total_PER(train_w2v, train_y))\n",
    "print(\"Test PER score: \", compute_total_PER(test_w2v, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
